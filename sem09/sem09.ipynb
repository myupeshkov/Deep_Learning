{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYqqG4qcp7oB"
   },
   "source": [
    "# Skip-gram Word2Vec\n",
    "\n",
    "## Хорошие ссылки для чтения\n",
    "\n",
    "\n",
    "* Полезный [общий взгляд](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) на Word2Vec\n",
    "* [Первые работы с Word2Vec](https://arxiv.org/pdf/1301.3781.pdf) \n",
    "* [Neural Information Processing Systems, paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) с улучшениями Word2Vec \n",
    "\n",
    "---\n",
    "## Word embeddings\n",
    "\n",
    "Когда вы имеете дело со словами в тексте, вам приходится анализировать десятки тысяч классов слов; по одному для каждого слова в словаре. Попытка однократно закодировать эти слова крайне неэффективна, потому что большинство значений в one-hot-encoding векторе будут установлены на ноль. Таким образом, умножение матриц, которое происходит между одним one-hot-encoding входным вектором и первым, скрытым слоем, приведет в основном к скрытым выходным данным с нулевым значением.\n",
    "\n",
    "<img src='https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/one_hot_encoding.png?raw=1' width=50%>\n",
    "\n",
    "Чтобы решить эту проблему и значительно повысить эффективность наших сетей, мы используем так называемые **эмбеддинги**. Эмбеддинги - это просто полностью связанный слой, как вы видели раньше. Мы называем этот слой вложенным слоем, а веса - вложенными весами. Мы пропускаем умножение в слой эмбеддингов, вместо этого напрямую извлекая значения скрытого слоя из весовой матрицы. Мы можем сделать это, потому что умножение one-hot-encoding вектора на матрицу возвращает строку матрицы, соответствующую индексу входного блока \"on\".\n",
    "\n",
    "<img src='https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/lookup_matrix.png?raw=1' width=50%>\n",
    "\n",
    "Вместо того, чтобы выполнять умножение матриц, мы используем матрицу весов в качестве таблицы поиска. Мы кодируем слова как целые числа, например, «сердце» кодируется как 958, «разум» как 18094. Затем, чтобы получить значения скрытого слоя для «сердца», вы просто берете 958-ю строку матрицы эмбеддингов. Этот процесс называется **поиском эмбеддинга**, а количество скрытых единиц - **измерением эмбеддинга**.\n",
    "\n",
    "<img src='https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/tokenize_lookup.png?raw=1' width=50%>\n",
    " \n",
    "Здесь нет ничего волшебного. Таблица эмбеддингов - это просто весовая матрица. Слой эмбеддинга - это просто скрытый слой. Поиск - это просто ярлык для умножения матриц. Таблица поиска обучается так же, как и любая матрица весов.\n",
    "\n",
    "Конечно, эмбеддинги используются не только для слов. Вы можете использовать их для любой модели, где у вас огромное количество классов. Конкретный тип модели под названием **Word2Vec** использует слой эмбеддингов для поиска векторных представлений слов, содержащих семантическое значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrg11LAsp7oB"
   },
   "source": [
    "---\n",
    "## Word2Vec\n",
    "\n",
    "Алгоритм Word2Vec находит гораздо более эффективные представления, находя векторы, представляющие слова. Эти векторы также содержат семантическую информацию о словах.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/context_drink.png?raw=1\" width=40%>\n",
    "\n",
    "Слова, которые появляются в похожих **контекстах**, таких как «кофе», «чай» и «вода», будут иметь векторы рядом друг с другом. Различные слова будут дальше друг от друга, а отношения могут быть представлены расстоянием в векторном пространстве.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/vector_distance.png?raw=1\" width=40%>\n",
    "\n",
    "\n",
    "Есть две архитектуры для реализации Word2Vec:\n",
    ">* CBOW (Continuous Bag-Of-Words) и \n",
    "* Skip-gram\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/word2vec_architectures.png?raw=1\" width=60%>\n",
    "\n",
    "В этой реализации мы будем использовать **архитектуру skip-gram**, потому что она работает лучше, чем CBOW. Здесь мы передаем слово и пытаемся предсказать слова, окружающие его в тексте. Таким образом, мы можем обучить сеть изучать представления слов, которые появляются в схожих контекстах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn4RsC-rp7oB"
   },
   "source": [
    "---\n",
    "## Loading Data\n",
    "\n",
    "Для начала скачаем данные\n",
    "\n",
    "1. Скачаем [dataset](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip); файл очищенного *текста статьи в Википедии* от Мэтта Махони.\n",
    "2. Расположим эти данные в  `data`.\n",
    "3. Затем удалим скачанный файл для экономии памяти\n",
    "\n",
    "После этого мы оставим только один файл в репозитории: `data/text8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkuGJuoqAFKs",
    "outputId": "e202b134-1f85-4d20-a181-6cb6c1722d82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-17 21:36:24--  https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.94.125\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.94.125|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31344016 (30M) [application/zip]\n",
      "Saving to: ‘text8.zip’\n",
      "\n",
      "text8.zip           100%[===================>]  29.89M  62.3MB/s    in 0.5s    \n",
      "\n",
      "2020-11-17 21:36:25 (62.3 MB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
      "\n",
      "--2020-11-17 21:36:25--  https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/utils.py\n",
      "Resolving github.com (github.com)... 140.82.112.4\n",
      "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘utils.py’\n",
      "\n",
      "utils.py                [ <=>                ] 109.50K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2020-11-17 21:36:26 (1.89 MB/s) - ‘utils.py’ saved [112132]\n",
      "\n",
      "Archive:  text8.zip\n",
      "  inflating: text8                   \n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip \n",
    "!unzip text8.zip\n",
    "!rm text8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o65mAtMVADv5",
    "outputId": "a6525cee-53e6-441a-95be-03fd82368b64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "# read in the extracted text file      \n",
    "with open('text8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print out the first 100 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck6w80Q5p7oD"
   },
   "source": [
    "## Pre-processing\n",
    "\n",
    "Здесь мы чистим текст, чтобы облегчить обучение. Это происходит из файла `utils.py`. Функция `preprocess` делает несколько вещей:\n",
    ">* Он преобразует любые знаки препинания в токены, поэтому точка заменяется на `<PERIOD>`. В этом наборе данных нет никаких периодов, но это поможет при решении других задач. \n",
    "* Он удаляет все слова, которые встречаются в наборе данных пять или *меньше* раз. Это значительно уменьшит проблемы, связанные с шумом в данных, и улучшит качество векторных представлений.\n",
    "* Он возвращает список слов в тексте.\n",
    "\n",
    "Это может занять несколько секунд, так как наш текстовый файл довольно большой. Если вы хотите написать свои собственные функции для этого материала, дерзайте!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aFUmJtPup7oD",
    "outputId": "86859698-2a99-4d3f-efdf-aaa9bdca7ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "# get list of words\n",
    "words = utils.preprocess(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dsOItB4Ip7oD",
    "outputId": "7a05c906-c8bd-4894-d7b8-d72392d9e5a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in text: 16680599\n",
      "Unique words: 63641\n"
     ]
    }
   ],
   "source": [
    "# print some stats about this word data\n",
    "print(\"Total words in text: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words)))) # `set` removes any duplicate words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC2mLtwip7oD"
   },
   "source": [
    "### Dictionaries\n",
    "\n",
    "Затем мы создаем два словаря для преобразования слов в целые числа и обратно (целые числа в слова). Это снова делается с помощью функции из файла `utils.py`. create_lookup_tables принимает список слов в тексте и возвращает два словаря.\n",
    ">* Целые числа присваиваются в порядке убывания частоты, поэтому самому частому слову («the») присваивается целое число 0, а следующему по частоте - 1 и так далее. \n",
    "\n",
    "Когда у нас есть словари, слова преобразуются в целые числа и сохраняются в списке int_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05eK3RZSp7oD",
    "outputId": "20aa95a5-92ee-4640-87ff-e647041c061b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580]\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "print(int_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5HDfby5p7oD"
   },
   "source": [
    "## Subsampling\n",
    "\n",
    "Часто встречающиеся слова, такие как «the», «of» и «for», не обеспечивают особого контекста для близлежащих слов. Если мы отбросим некоторые из них, мы сможем удалить часть шума из наших данных и взамен получить более быстрое обучение и лучшее представление. Этот процесс Миколов назвал субдискретизацией. Для каждого слова $w_i$ в обучающем наборе мы отбрасываем его с вероятностью, равной\n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "где $t$ - пороговый параметр, а $f(w_i)$ - частота слова $w_i$ в общем наборе данных.\n",
    "\n",
    "$$ P(0) = 1 - \\sqrt{\\frac{1*10^{-5}}{1*10^6/16*10^6}} = 0.98735 $$\n",
    "\n",
    "Это остается вам в качестве упражнения. \n",
    "\n",
    "> **Упражнение:** Реализуйте подвыборку для слов в `int_words`. То есть пройдите через `int_words` и отбросьте каждое слово с вероятностью $ P (w_i) $, показанной выше. Обратите внимание, что $ P (w_i) $ - это вероятность того, что слово будет отброшено. Назначьте данные с подвыборкой для `train_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EenbWXACp7oD",
    "outputId": "11704915-5618-429b-9b32-b65c8f8fc93f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5233, 303)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "print(list(word_counts.items())[0])  # dictionary of int_words, how many times they appear\n",
    "\n",
    "# discard some frequent words, according to the subsampling equation\n",
    "# create a new list of words for training\n",
    "train_words = None\n",
    "\n",
    "#print(train_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfEOWrhcp7oD"
   },
   "source": [
    "## Making batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLCZzZVrp7oD"
   },
   "source": [
    "Теперь, когда наши данные в хорошей форме, нам нужно привести их в правильную форму, чтобы передать их в нашу сеть. В архитектуре skip-gram для каждого слова в тексте мы хотим определить окружающий _context_ и захватить все слова в окне вокруг этого слова с размером $ C $. \n",
    "\n",
    "Из [статьи.](https://arxiv.org/pdf/1301.3781.pdf): \n",
    "\n",
    "\"Поскольку более далекие слова обычно меньше связаны с текущим словом, чем близкие к нему, мы придаем меньший вес удаленным словам, отбирая меньшее количество из этих слов в наших обучающих примерах ... Если мы выберем $ C = 5 $, то для каждого обучающего слова мы выбираем случайным образом число $ R $ в диапазоне $ [1: C] $, а затем используем слова $ R $ из предыдущих слов и слова $ R $ из будущего текущего слова в качестве правильных меток.\"\n",
    "\n",
    "> **Упражнение:** Реализуйте функцию get_target, которая получает список слов, индекс и размер окна, а затем возвращает список слов в окне вокруг индекса. Обязательно используйте алгоритм, описанный выше, где вы выбрали случайное количество слов из окна.\n",
    "\n",
    "Скажем, у нас есть вход, и нас интересует токен idx = 2, `741`: \n",
    "```\n",
    "[5233, 58, 741, 10571, 27349, 0, 15067, 58112, 3580, 58, 10712]\n",
    "```\n",
    "\n",
    "Для R = 2 функция get_target должна возвращать список из четырех значений:\n",
    "```\n",
    "[5233, 58, 10571, 27349]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmzEBIfdp7oD"
   },
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    # implement this function\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "259vdIDVp7oD",
    "outputId": "013606c4-2498-4a2d-e36e-27b82bedb2d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Target:  [0, 1, 2, 3, 4, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# test your code!\n",
    "\n",
    "# run this cell multiple times to check for random window selection\n",
    "int_text = [i for i in range(10)]\n",
    "print('Input: ', int_text)\n",
    "idx=5 # word index of interest\n",
    "\n",
    "target = get_target(int_text, idx=idx, window_size=5)\n",
    "print('Target: ', target)  # you should get some indices around the idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucXKA7zxp7oD"
   },
   "source": [
    "### Generating Batches \n",
    "\n",
    "Вот функция-генератор, которая возвращает пакеты входных и целевых данных для нашей модели, используя функцию get_target, описанную выше. Идея в том, что он берет слова `batch_size` из списка слов. Затем для каждого из этих пакетов слова отображаются в окне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HghTPaSgp7oD"
   },
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TJ2c2XDp7oD",
    "outputId": "87ddcf50-f9ac-4cba-a614-12cb59c9d73c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]\n",
      "y\n",
      " [1, 2, 3, 0, 2, 3, 0, 1, 3, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "int_text = [i for i in range(20)]\n",
    "x,y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
    "\n",
    "print('x\\n', x)\n",
    "print('y\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeqYgmh2p7oD"
   },
   "source": [
    "## Building the graph\n",
    "\n",
    "Ниже представлена ​​примерная схема общей структуры нашей сети.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/skip_gram_arch.png?raw=1\" width=60%>\n",
    "\n",
    ">* Входные слова передаются как пакеты токенов входных слов. \n",
    "* Они входят в скрытый слой линейных единиц (наш слой эмбеддинга). \n",
    "* Затем, наконец, в выходной слой softmax. \n",
    "\n",
    "Мы будем использовать слой softmax, чтобы делать предсказания о контекстных словах, как обычно.\n",
    "\n",
    "Идея состоит в том, чтобы обучить матрицу весов слоя эмбеддингов и найти эффективные представления для наших слов. Мы можем отбросить слой softmax, потому что нам не нужно делать прогнозы с помощью этой сети. Нам просто нужна матрица эмбеддингов, чтобы мы могли использовать ее в _других_ сетях, которые мы строим с использованием этого набора данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hS3IjZHDp7oD"
   },
   "source": [
    "---\n",
    "## Validation\n",
    "\n",
    "Здесь нужно создать функцию, которая поможет нам наблюдать за нашей моделью в процессе обучения. Нужно выбрать несколько общих слов и несколько необычных слов. Затем мы распечатаем ближайшие к ним слова, используя косинусное сходство:\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/two_vectors.png?raw=1\" width=30%>\n",
    "\n",
    "$$\n",
    "\\mathrm{similarity} = \\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|}\n",
    "$$\n",
    "\n",
    "\n",
    "Мы можем закодировать слова проверки как векторы $\\vec{a}$, используя таблицу эмбеддингов, а затем вычислить сходство с каждым вектором слов $\\vec{b}$ в таблице эмбеддингов. Имея сходство, мы можем распечатать проверочные слова и слова в нашей матрице эмбеддингов, семантически похожие на эти слова. Это хороший способ проверить, объединяет ли наша таблица эмбеддингов слова с похожими семантическими значениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCEQG5RJp7oD"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
    "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
    "        Here, embedding should be a PyTorch embedding module.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we're calculating the cosine similarity between some random words and \n",
    "    # our embedding vectors. With the similarities, we can look at what words are\n",
    "    # close to our random words.\n",
    "    \n",
    "    # sim = (a . b) / |a||b|\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
    "        \n",
    "    return valid_examples, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guJsEYdBp7oD"
   },
   "source": [
    "## SkipGram model\n",
    "\n",
    "Определите и обучите модель SkipGram. \n",
    "> Вам нужно будет определить [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) и последний выходной слой softmax.\n",
    "\n",
    "Слой внедрения принимает ряд входных данных:\n",
    "* **num_embeddings** – размер словаря эмбеддингов или сколько строк вы хотите в матрице эмбеддингов\n",
    "* **embedding_dim** – размер каждого вектора эмбеддинга; размер эмбеддинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTemhFnMp7oD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8h3wgY_p7oD"
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed):\n",
    "        super().__init__()\n",
    "        \n",
    "        # complete this SkipGram model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # define the forward behavior\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCVd0yaXp7oD"
   },
   "source": [
    "### Training\n",
    "\n",
    "**Обратите внимание: поскольку мы применили функцию softmax к выходным данным нашей модели, мы используем NLLLoss**, а не перекрестную энтропию. Это потому, что Softmax в сочетании с NLLLoss = CrossEntropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fg9IEgfZp7oD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "embedding_dim=300 # you can change, if you want\n",
    "\n",
    "model = SkipGram(len(vocab_to_int), embedding_dim).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "print_every = 500\n",
    "steps = 0\n",
    "epochs = 5\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # get input and target batches\n",
    "    for inputs, targets in get_batches(train_words, 512):\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        log_ps = model(inputs)\n",
    "        loss = criterion(log_ps, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if steps % print_every == 0:                  \n",
    "            # getting examples and similarities      \n",
    "            valid_examples, valid_similarities = cosine_similarity(model.embed, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6) # topk highest similarities\n",
    "            \n",
    "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
    "            print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkpRD4A8p7oD"
   },
   "source": [
    "## Visualizing the word vectors\n",
    "\n",
    "Ниже мы будем использовать T-SNE, чтобы визуализировать, как наши многомерные словесные векторы группируются вместе. Прочитать [эту статью](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)чтобы узнать больше о T-SNE и других способах визуализации многомерных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoYaca7xp7oD"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bs-ZhlbPp7oD"
   },
   "outputs": [],
   "source": [
    "# getting embeddings from the embedding layer of our model, by name\n",
    "embeddings = model.embed.weight.to('cpu').data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za8ne2ADp7oE"
   },
   "outputs": [],
   "source": [
    "viz_words = 600\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1oitQiqp7oE"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPtklc_2p7oE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Verteeva_Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
