{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Tuple, Dict, List, Union\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"media/rnn-transformer-meme.png\" style=\"width: 500px;\"/></center>"
   ]
  },
  {
   "source": [
    "## Что же такое этот Transformer?\n",
    "Transformer - модель полностью основанная только на Attention механизме.\n",
    "<table>\n",
    "    <td> <img src=\"media/transformer-architecture.png\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"media/dot-product-multi-head.png\" style=\"width: 600px;\"/> </td>\n",
    "</table>\n",
    "$B$ - batch size, $T$ - sequence length, $E$ - embedding size\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    &Input &&\\sim &&B \\times T \\times E \\\\\n",
    "    &Q = InputW^{Q} &&\\sim &&B \\times T \\times QK_{dim} \\\\\n",
    "    &K = InputW^{K} &&\\sim &&B \\times T \\times QK_{dim} \\\\\n",
    "    &V = InputW^{V} &&\\sim &&B \\times T \\times V_{dim} \\\\\n",
    "    &QK^{T} &&\\sim &&B \\times T \\times T\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    Self \\hbox{-} Attention(Q,\\,K,\\,V) = Softmax\\biggl(\\frac{QK^{T}}{\\sqrt{QK_{dim}}}\\biggr)V\n",
    "\\end{equation}\n",
    "$$\n",
    "Где $W^{Q} \\in R^{E \\times QK_{dim}},\\, W^{K} \\in R^{E \\times QK_{dim}},\\, W^{V} \\in R^{E \\times V_{dim}}$\n",
    "\n",
    "В качестве Norm используют [LayerNorm by Hinton](https://arxiv.org/abs/1607.06450), так как он значительно лучше работает для NLP задач и не зависит от размера Batch, благодаря чему можно делать онлайн-обучение. Также для LayerNorm нет необходимости делать скользящее средней по expectation и variance.\n",
    "Теперь порисуем где-нибудь модель и напишем псевдокод для каждой части."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преимущества Transformer\n",
    "<center><img src=\"media/lstm-schmidhuber-meme.jpg\" style=\"width: 500px;\"/></center>\n",
    "\n",
    "1. Быстрее.\n",
    "2. Проще распаралелить.\n",
    "3. Лучше работает с длинными последовательностями (так как расстояние от любого другого токена равно `O(1)`, в то время как у RNN `O(n)`).\n",
    "4. Интерпретируемость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Напишем свой MultiHead Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionOutput(NamedTuple):\n",
    "    \"\"\"Result of MultiHeadAttention Module call.\"\"\"\n",
    "\n",
    "    values: torch.Tensor\n",
    "    attention: torch.Tensor\n",
    "\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Compute Multi-Head Attention like in \"Attention Is All You Need\" paper.\n",
    "    For simplicity assume that value dimension equals query and key dimension.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_heads : `int`, required\n",
    "        Number of heads for Self-Attention.\n",
    "    hidden_size : `int`, required\n",
    "        Hidden size for projection in Self-Attention.\n",
    "    bias : `bool`, optional (default = `True`)\n",
    "        Whether to include bias for projection or not.\n",
    "    dropout : `float`, optional (default = `0.1`)\n",
    "        Dropout probability for Self-Attention after softmax.\n",
    "    output_size : `int`, optional (default = `None`)\n",
    "        Size for output projection. If None hidden size is used.\n",
    "    attention_fill_value : `float`, optional (default = `1e-32`)\n",
    "        Fill value for attention before softmax if mask is passed if forward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_size: int,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.1,\n",
    "        output_size: int = None,\n",
    "        attention_fill_value: float = 1e-32\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        self.attn_size = hidden_size // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.projections = torch.nn.ModuleDict({\n",
    "            \"query\": torch.nn.Linear(hidden_size, hidden_size, bias=bias),\n",
    "            \"key\": torch.nn.Linear(hidden_size, hidden_size, bias=bias),\n",
    "            \"value\": torch.nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        })\n",
    "        self.output = torch.nn.Linear(hidden_size, output_size or hidden_size)\n",
    "        self.attention_fill_value = attention_fill_value\n",
    "        if dropout:\n",
    "            self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = lambda x: x\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor = None\n",
    "    ) -> MultiHeadAttentionOutput:\n",
    "        # query ~ (batch size, seq length, hidden size)\n",
    "        # key ~ (batch size, seq length, hidden size)\n",
    "        # value ~ (batch size, seq length, hidden size)\n",
    "        # mask ~ (batch size, seq length)\n",
    "        # Because we decided to keep the model simple\n",
    "        # then query.size = key.size = value.size\n",
    "        original_size = value.size()\n",
    "        # 1) Linear projections in batch from\n",
    "        # (batch size, seq length, hidden size) => (batch size, num heads, seq length, attn size).\n",
    "        # TODO: YOUR CODE HERE\n",
    "        # 2) Apply self-attention.\n",
    "        # output ~ (batch size, num heads, seq length, attn size)\n",
    "        # attn ~ (batch size, num heads, seq length, seq length)\n",
    "        # TODO: YOUR CODE HERE\n",
    "        # 3) Rearrange back to normal.\n",
    "        # output ~ (batch size, seq length, hidden size)\n",
    "        # TODO: YOUR CODE HERE\n",
    "        return MultiHeadAttentionOutput(torch.Tensor(), torch.Tensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зафиксировать seed и посмотрим работает ли))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randn(10, 15, 256)\n",
    "module = MultiHeadAttention(num_heads=4, hidden_size=256, dropout=0.0, bias=False)\n",
    "module(tensor, tensor, tensor).values.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну а теперь проверим с PyTorch MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.activation import MultiheadAttention as MultiheadAttentionTorch\n",
    "# Instantiate Module\n",
    "torch_attention = MultiheadAttentionTorch(embed_dim=256, num_heads=4, dropout=0.0, bias=False)\n",
    "# Change parameters\n",
    "torch_attention._qkv_same_embed_dim = False\n",
    "# Projections\n",
    "torch_attention.q_proj_weight = module.projections[\"query\"].weight\n",
    "torch_attention.k_proj_weight = module.projections[\"key\"].weight\n",
    "torch_attention.v_proj_weight = module.projections[\"value\"].weight\n",
    "torch_attention.out_proj = module.output\n",
    "# Permute tensor because PyTorch MultiheadAttention\n",
    "# accepts (sequence length, batch size, embedding size)\n",
    "torch_attn_tensor = tensor.permute(1, 0, 2).contiguous()\n",
    "assert torch.allclose(\n",
    "    torch_attention(torch_attn_tensor, torch_attn_tensor, torch_attn_tensor)[0].permute(1, 0, 2),\n",
    "    module(tensor, tensor, tensor).values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделать модель на основе Transformer в PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные для NMT French-English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data dir if not exists\n",
    "!mkdir -p data\n",
    "# Load dataset\n",
    "!wget https://www.manythings.org/anki/fra-eng.zip -O data/fra-eng.zip\n",
    "# Unarchive dataset\n",
    "!tar -xvf data/fra-eng.zip --directory data/\n",
    "# Rename for build_dataset to work\n",
    "!mv data/fra.txt data/eng-fra.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже код для предобработки данных. Не обращайте на него внимание, он взят из [туториала PyTorch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_dataset\n",
    "\n",
    "\n",
    "build_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовим LabelEncoder и Dataset в стилье PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "SOS = \"<start>\"\n",
    "EOS = \"<end>\"\n",
    "PAD = \"<pad>\"\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Vocabulry construct label encoding (token -> index)\n",
    "    and labeld decoding (index -> token) based on datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **datasets) -> None:\n",
    "        self._word2index = defaultdict(dict)\n",
    "        self._index2word = defaultdict(dict)\n",
    "        self._setup_indexers()\n",
    "        for name, dataset in datasets.items():\n",
    "            self._iterate_dataset(name, dataset)\n",
    "\n",
    "    @property\n",
    "    def word2index(self):\n",
    "        return self._word2index\n",
    "\n",
    "    @property\n",
    "    def index2word(self):\n",
    "        return self._index2word\n",
    "\n",
    "    def get_size(self, field: str) -> int:\n",
    "        return len(self._word2index[field])\n",
    "\n",
    "    def _iterate_dataset(self, name: str, dataset: Dataset) -> None:\n",
    "        for sample in tqdm(dataset, desc=f\"Building vocab from {name}\", total=len(dataset)):\n",
    "            self._iterate_sample(sample)\n",
    "\n",
    "    def _iterate_sample(self, sample: Dict[str, List[str]]) -> None:\n",
    "        for field, sentence in sample.items():\n",
    "            for word in filter(\n",
    "                lambda x: x not in self._word2index[field], sentence\n",
    "            ):\n",
    "                index = len(self._word2index[field])\n",
    "                self._word2index[field][word] = index\n",
    "                self._index2word[field][index] = word\n",
    "\n",
    "    def _setup_indexers(self) -> None:\n",
    "        for token_type in [PAD, SOS, EOS]:\n",
    "            index = len(self._word2index[\"source\"])\n",
    "            self._word2index[\"source\"][token_type] = index\n",
    "            self._word2index[\"target\"][token_type] = index\n",
    "            self._index2word[\"source\"][index] = token_type\n",
    "            self._index2word[\"target\"][index] = token_type\n",
    "\n",
    "\n",
    "class NMTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for NMT task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : `str`, required\n",
    "        Path to dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str) -> None:\n",
    "        self._dataset = []\n",
    "        self._indexer = None\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            for sample in tqdm(file, desc=f\"Reading dataset at {data_path} in memory\"):\n",
    "                source, target = sample.rstrip().split('\\t')\n",
    "                self._dataset.append({\n",
    "                    \"source\": source.split() + [EOS],\n",
    "                    \"target\": [SOS] + target.split() + [EOS],\n",
    "                })\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, List[str]]:\n",
    "        return {\n",
    "            k: [self._indexer[k][x] if self._indexer else x for x in v]\n",
    "            for k, v in self._dataset[idx].items()\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def index_with(self, vocab: Vocabulary) -> None:\n",
    "        self._indexer = vocab.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# collate_fn в DataLoader вызывается чуть ли не в самом конце сборки одного батча\n",
    "# поэтому это отличный способ сделать дополнительную предобработку данных\n",
    "# в нашем случае нам необходимо сделать padding и сделать маску\n",
    "class CollateBatch:\n",
    "    \"\"\"\n",
    "    Collate Function for DataLoader as Class to perform postprocessing.\n",
    "    We better use classes because it is better for code structure\n",
    "    and much more convenient to build custom functions for batch of samples such pin_memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    instances : `List[Dict[str, List[int]]]`\n",
    "        List of samples as dicts from PyTorch Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, instances: List[Dict[str, List[int]]]) -> None:\n",
    "        batch = self._form_batch(instances)\n",
    "        # Pad sentences as each sentence is of different size\n",
    "        self.source_tokens = pad_sequence([torch.Tensor(x) for x in batch['source']], batch_first=True).long()\n",
    "        self.target_tokens = pad_sequence([torch.Tensor(x) for x in batch['target']], batch_first=True).long()\n",
    "        # Construct mask to identify padding\n",
    "        self.source_mask = self.source_tokens.ne(0)\n",
    "        self.target_mask = self.target_tokens.ne(0)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        \"\"\"Pin memory for fast data transfer on CUDA.\"\"\"\n",
    "        self.__dict__ = {\n",
    "            prop: tensor.pin_memory()\n",
    "            for prop, tensor in self.__dict__.items()\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def to_device(\n",
    "        self,\n",
    "        device: Union[str, torch.device],\n",
    "        **extra_params\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Helper function to send batch to device and convert it to dict.\"\"\"\n",
    "        return {\n",
    "            prop: tensor.to(device=device, **extra_params)\n",
    "            for prop, tensor in self.__dict__.items()\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _form_batch(instances: List[Dict[str, List[int]]]) -> Dict[str, List[List]]:\n",
    "        \"\"\"Consturct normal batched data as dict from list of dicts.\"\"\"\n",
    "        tensor_dict = defaultdict(list)\n",
    "        for instance in instances:\n",
    "            for field, tensor in instance.items():\n",
    "                tensor_dict[field].append(tensor)\n",
    "        return tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building datasets\n",
    "train_dataset = NMTDataset('./data/train.txt')\n",
    "validation_dataset = NMTDataset('./data/test.txt')\n",
    "# Building Vocabulry to convert words to indecies sequence\n",
    "vocab = Vocabulary(\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=validation_dataset\n",
    ")\n",
    "# Index datasets with Vocabulary\n",
    "train_dataset.index_with(vocab)\n",
    "validation_dataset.index_with(vocab)\n",
    "# Building DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=CollateBatch,\n",
    "    shuffle=True,\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    dataset=validation_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=CollateBatch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напишем модель на основе LSTM для NMT задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class LSTMEncoderOutput(NamedTuple):\n",
    "    final_state: torch.Tensor\n",
    "    cell_state: torch.Tensor\n",
    "\n",
    "\n",
    "class LSTMEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Decode text data with LSTM Module.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : `int`, required\n",
    "        Hidden size of input tokens.\n",
    "    hidden_size : `int`, required\n",
    "        Hidden size for LSTM.\n",
    "    num_layers : `int`, optional (default = `1`)\n",
    "        Number of stacked LSTM modules.\n",
    "    dropout : `int`, optional (default = `0.2`)\n",
    "        Dropout for LSTM module.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.2\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._encoder = torch.nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, source_tokens: torch.Tensor, mask: torch.Tensor) -> LSTMEncoderOutput:\n",
    "        # source_tokens ~ (batch size, sequence length, hidden size)\n",
    "        # mask ~ (batch size, sequence length)\n",
    "        # 1) Pack sequence to skip processing of padding in tokens tensor\n",
    "        # (LSTM returns 0 on padding)\n",
    "        # Get lengths of each sequence\n",
    "        lenghts = mask.long().sum(dim=-1)\n",
    "        # Pack sequence to skip padding in RNN\n",
    "        packed_sequence_input = pack_padded_sequence(\n",
    "            input=source_tokens,\n",
    "            lengths=lenghts.data.tolist(),\n",
    "            batch_first=True,\n",
    "            # Let pack_padded do the sorting for ourselves\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        # 2) Encode PackedSequence\n",
    "        padded_encoded_sequence, (final_hidden_state, cell_state) = self._encoder(packed_sequence_input)\n",
    "        # 3) Unpack encoded sequence\n",
    "        # encoded_sequence ~ (batch size, sequence length, LSTM hidden size * 2)\n",
    "        encoded_sequence = pad_packed_sequence(padded_encoded_sequence, batch_first=True)\n",
    "        # Take average over frist dimension as it returns both forward and backward paths of LSTM\n",
    "        return LSTMEncoderOutput(\n",
    "            final_hidden_state.mean(0).unsqueeze(0),\n",
    "            cell_state.mean(0).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "    def get_input_size(self):\n",
    "        return self._input_size\n",
    "\n",
    "    def get_output_size(self):\n",
    "        # Return bidirectional LSTM so mul by 2\n",
    "        return self._hidden_size * 2\n",
    "\n",
    "\n",
    "class LSTMDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Decode sequence for NMT task with LSTM Module.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : `int`, required\n",
    "        Hidden size of input tokens.\n",
    "    hidden_size : `int`, required\n",
    "        Hidden size for LSTM.\n",
    "    num_layers : `int`, optional (default = `1`)\n",
    "        Number of stacked LSTM modules.\n",
    "    dropout : `int`, optional (default = `0.2`)\n",
    "        Dropout for LSTM module.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.2\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._decoder = torch.nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        target_tokens: torch.Tensor,\n",
    "        encoder_output: LSTMEncoderOutput,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # target_tokens ~ (batch size, sequence length, hidden size)\n",
    "        # mask ~ (batch size, sequence length)\n",
    "        # 1) Pack sequence to skip processing of padding in tokens tensor\n",
    "        # (LSTM returns 0 on padding)\n",
    "        lenghts = mask.long().sum(dim=-1)\n",
    "        # Pack sequence to skip padding in RNN\n",
    "        packed_sequence_input = pack_padded_sequence(\n",
    "            input=target_tokens,\n",
    "            lengths=lenghts.data.tolist(),\n",
    "            batch_first=True,\n",
    "            # Let pack_padded do the sorting for ourselves\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        # 2) Encode PackedSequence\n",
    "        padded_decoded_sequence = self._decoder(packed_sequence_input, encoder_output)[0]\n",
    "        # 3) Unpack encoded sequence\n",
    "        # decoded_sequence ~ (batch size, sequence lenght, LSTM hidden size)\n",
    "        decoded_sequence, _ = pad_packed_sequence(padded_decoded_sequence, batch_first=True)\n",
    "        return decoded_sequence\n",
    "\n",
    "    def get_input_size(self):\n",
    "        return self._input_size\n",
    "\n",
    "    def get_output_size(self):\n",
    "        return self._hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напишем модель на основе Transformer для NMT задачи\n",
    "Формулы для Positional Encoding, где $i \\in [0, \\frac{d}{2}]$ и $d$ - это размерность входного токена (размерность эмбеддинга)\n",
    "$$\n",
    "\\begin{gather}\n",
    "    PE_{t,\\,2i} = sin(t/10000^{\\frac{2i}{d}}),\\\\\n",
    "    PE_{t,\\,2i + 1} = cos(t/10000^{\\frac{2i}{d}})\n",
    "\\end{gather}\n",
    "$$\n",
    "Или по-другому, где $c_{i}$ - это некоторая константа перед $t$'тым токеном (для Transformer $c_{i} = 1 / 10000^{\\frac{2i}{d}}$)\n",
    "$$\n",
    "\\begin{equation}\n",
    "    PE_{t} =\n",
    "    \\begin{bmatrix}\n",
    "        sin(c_{0}t)\\\\\n",
    "        cos(c_{0}t)\\\\\n",
    "        .\\\\\n",
    "        .\\\\\n",
    "        .\\\\\n",
    "        sin(c_{\\frac{d}{2} - 1}t)\\\\\n",
    "        cos(c_{\\frac{d}{2} - 1}t)\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderOutput(NamedTuple):\n",
    "    memory: torch.Tensor\n",
    "    mask: torch.Tensor\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding for Transformer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_size : `int`, required\n",
    "        Hidden size of positional encoding.\n",
    "        Must match hidden size of input tokens.\n",
    "    dropout : `float`, required\n",
    "        Dropout probability after positional encoding addition.\n",
    "        If None dropout is not considered.\n",
    "    max_len : `int`, optional (default = `5000`)\n",
    "        Maximum sequence length to construct Positional Encoding. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, dropout: float, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        # TODO: YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: YOUR CODE HERE\n",
    "        return torch.Tensor()\n",
    "\n",
    "\n",
    "# PyTorch Transformer is somewhat weirdly implemented\n",
    "# so you better write one yourself but for now let's use PyTorch Transformer\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encode input tokens with Transformer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : `int`, required\n",
    "        Hidden size of input tokens.\n",
    "    num_layers : `int`, required\n",
    "        Number of stacked Transformers.\n",
    "    feedforward_hidden_dim : `int`, optional (default = `2048`)\n",
    "        Hidden size for feedforward layer in Transformer.\n",
    "    num_attention_heads : `int`, optional (default = `8`)\n",
    "        Number of attention heads in Transformer.\n",
    "    dropout_prob : `float`, optional (default = `0.1`)\n",
    "        Dropout probability for Transformer.\n",
    "    activation : `str`, optional (default = `\"relu\"`)\n",
    "        The activation function of intermediate layer, relu or gelu.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        num_layers: int,\n",
    "        feedforward_hidden_dim: int = 2048,\n",
    "        num_attention_heads: int = 8,\n",
    "        dropout_prob: float = 0.1,\n",
    "        activation: str = \"relu\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        layer = torch.nn.TransformerEncoderLayer(\n",
    "            d_model=input_size,\n",
    "            nhead=num_attention_heads,\n",
    "            dim_feedforward=feedforward_hidden_dim,\n",
    "            dropout=dropout_prob,\n",
    "            activation=activation,\n",
    "        )\n",
    "        self._transformer = torch.nn.TransformerEncoder(layer, num_layers)\n",
    "        self._positional_encoding = PositionalEncoding(\n",
    "            hidden_size=input_size,\n",
    "            dropout=None,\n",
    "        )\n",
    "        self._input_size = input_size\n",
    "\n",
    "    def forward(self, source_tokens: torch.Tensor, mask: torch.Tensor) -> TransformerEncoderOutput:\n",
    "        # source_tokens ~ (batch size, sequence length, hidden size)\n",
    "        # mask ~ (batch size, sequence length)\n",
    "        tokens = self._positional_encoding(source_tokens)\n",
    "        # For some reason the torch transformer expects the shape (sequence, batch, features), not the more\n",
    "        # familiar (batch, sequence, features), so we have to fix it.\n",
    "        # tokens ~ (sequence length, batch size, hidden size)\n",
    "        tokens = tokens.permute(1, 0, 2)\n",
    "        # For some other reason, the torch transformer takes the mask backwards.\n",
    "        tokens_mask = ~mask\n",
    "        # output ~ (sequence length, batch size, hidden size)\n",
    "        output = self._transformer(tokens, src_key_padding_mask=tokens_mask)\n",
    "        # output ~ (batch size, sequence length, hidden size)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        return TransformerEncoderOutput(output, mask)\n",
    "\n",
    "    def get_input_size(self) -> int:\n",
    "        return self._input_size\n",
    "\n",
    "    def get_output_size(self) -> int:\n",
    "        return self._input_size\n",
    "\n",
    "\n",
    "class TransformerDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Decode sequence for NMT task with Transformer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : `int`, required\n",
    "        Hidden size of input tokens.\n",
    "    num_layers : `int`, required\n",
    "        Number of stacked Transformers.\n",
    "    feedforward_hidden_dim : `int`, optional (default = `2048`)\n",
    "        Hidden size for feedforward layer in Transformer.\n",
    "    num_attention_heads : `int`, optional (default = `8`)\n",
    "        Number of attention heads in Transformer.\n",
    "    dropout_prob : `float`, optional (default = `0.1`)\n",
    "        Dropout probability for Transformer.\n",
    "    activation : `str`, optional (default = `\"relu\"`)\n",
    "        The activation function of intermediate layer, relu or gelu.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        num_layers: int,\n",
    "        feedforward_hidden_dim: int = 2048,\n",
    "        num_attention_heads: int = 8,\n",
    "        dropout_prob: float = 0.1,\n",
    "        activation: str = \"relu\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        layer = torch.nn.TransformerDecoderLayer(\n",
    "            d_model=input_size,\n",
    "            nhead=num_attention_heads,\n",
    "            dim_feedforward=feedforward_hidden_dim,\n",
    "            dropout=dropout_prob,\n",
    "            activation=activation\n",
    "        )\n",
    "        self._transformer = torch.nn.TransformerDecoder(layer, num_layers=num_layers)\n",
    "        self._input_size = input_size\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        target_tokens: torch.Tensor,\n",
    "        encoder_output: TransformerEncoderOutput,\n",
    "        mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # target_tokens ~ (batch size, sequence length, hidden size)\n",
    "        # mask ~ (batch size, sequence length)\n",
    "        # For some reason the torch transformer expects the shape (sequence, batch, features), not the more\n",
    "        # familiar (batch, sequence, features), so we have to fix it.\n",
    "        # target_tokens ~ (sequence length, batch size, hidden size)\n",
    "        # memory ~ (sequence length, batch size, hidden size)\n",
    "        target_tokens = target_tokens.permute(1, 0, 2)\n",
    "        memory = encoder_output.memory.permute(1, 0, 2)\n",
    "        # For some other reason, the torch transformer takes the mask backwards.\n",
    "        target_mask = ~mask\n",
    "        memory_mask = ~encoder_output.mask\n",
    "        # output ~ (sequence length, batch size, hidden size)\n",
    "        output = self._transformer(\n",
    "            target_tokens, memory,\n",
    "            tgt_key_padding_mask=target_mask,\n",
    "            memory_key_padding_mask=memory_mask,\n",
    "        )\n",
    "        # output ~ (batch size, sequence length, hidden size)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        return output\n",
    "\n",
    "    def get_input_size(self) -> int:\n",
    "        return self._input_size\n",
    "\n",
    "    def get_output_size(self) -> int:\n",
    "        return self._input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напишем Loss под нашу задачу\n",
    "\n",
    "До этого мы использовали дефолтные лоссы в PyTorch, но теперь нам нужно написать свой лосс, так как мы хотим посчитать `CrossEntropyLoss` по тензору из 3 размерностей, в то время как `PyTorch CrossEntropyLoss` принимает тензоры размерности 2.\n",
    "\n",
    "Возможно в примерах в интернете вы виделе как пишут свой лосс через `torch.autograd.Function`. Это хорошая практики и отличная тренировка писать ручками свой Backward, однако такой подход лучше всего подходит, когда у вас сложный лосс и там возникают for. Для нашего лосс это не нужно, поэтому сдалем всё через знакомый нам `torch.nn.Module`.\n",
    "\n",
    "Очевидное решение, это просто сделать reshape и соединить `batch size` и `sequence length`, однако в таком случае мы будет считать лосс ещё и по `padding`, что нам совсем не нужно.\n",
    "\n",
    "Поэтому напишем свой `CrossEntropyLoss`, который не будет учитывать `padding`. Его идея будет заключаться в том, чтобы взять логиты под индексами, которые не являются `padding`. Для этого очень хорошо подходит операция [torch.gather](https://pytorch.org/docs/stable/generated/torch.gather.html#torch-gather), которая буквально переводится как \"собрать\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceNLLLoss(torch.nn.Module):\n",
    "    \"\"\"NLL Loss for NMT task that ignores computing loss on paddding.\"\"\"\n",
    "    def __init__(self, size_average: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self._size_average = size_average\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        logits: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        weights: torch.FloatTensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # logits ~ (batch size, sequence length, num_classes)\n",
    "        # target ~ (batch size, sequence length)\n",
    "        # weights ~ (batch size, sequence length)\n",
    "        non_batch_dims = tuple(range(1, weights.dim()))\n",
    "        # weights_batch_sum  (batch size,)\n",
    "        weights_batch_sum = weights.sum(dim=non_batch_dims)\n",
    "        # logits_flat ~ (batch size * sequence length, num_classes)\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        log_probs_flat = torch.log_softmax(logits_flat, dim=-1)\n",
    "        # targets_flat ~ (batch size * sequence length, 1)\n",
    "        targets_flat = target.view(-1, 1).long()\n",
    "        # nll_loss ~ (batch size * sequence length,)\n",
    "        # Gather numbers from log_probs_flat in targets_flat indices\n",
    "        nll_loss = -torch.gather(log_probs_flat, dim=1, index=targets_flat)\n",
    "        # nll_loss ~ (batch size, sequence length)\n",
    "        nll_loss = nll_loss.view(*target.size()) * weights\n",
    "        # per_batch_nll_loss ~ (batch size,)\n",
    "        per_batch_nll_loss = nll_loss.sum(non_batch_dims) / torch.clamp(weights_batch_sum, min=1e-13)\n",
    "        if self._size_average:\n",
    "            num_non_empty_sequences = torch.clamp(weights_batch_sum.gt(0).float().sum(), min=1e-13)\n",
    "            return per_batch_nll_loss.sum() / num_non_empty_sequences\n",
    "        else:\n",
    "            return per_batch_nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напишем в общем виде модель для задачи NMT, которая будет поддерживать общий API моделей LSTM и Transformer описанных выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model for NMT task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : `Vocabulary`, required\n",
    "        Vocabulary with label encoding and label decoding constructed on datasets.\n",
    "    embeddings : `torch.nn.ModuleDict`, required\n",
    "        Torch ModuleDict of embeddings. It should have source and target keys.\n",
    "    encoder : `torch.nn.Module`, required\n",
    "        Encoder module for input tokens.\n",
    "    decoder : `torch.nn.Module`, required\n",
    "        Decoder module for encoder output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: Vocabulary,\n",
    "        embeddings: torch.nn.ModuleDict,\n",
    "        # Better use seperate Interface for encoder and decoder\n",
    "        # but for now let's keep it simple stupid and use torch.nn.Module\n",
    "        encoder: torch.nn.Module,\n",
    "        decoder: torch.nn.Module,\n",
    "        device: str,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        self._embeddings = embeddings\n",
    "        self._encoder = encoder\n",
    "        self._decoder = decoder\n",
    "        self._projection = torch.nn.Linear(\n",
    "            in_features=self._decoder.get_output_size(),\n",
    "            out_features=vocab.get_size(\"target\")\n",
    "        )\n",
    "        self._loss = SequenceNLLLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        source_tokens: torch.Tensor,\n",
    "        target_tokens: torch.Tensor,\n",
    "        source_mask: torch.Tensor,\n",
    "        target_mask: torch.Tensor,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # source_tokens ~ (batch size, sequence length)\n",
    "        # target_tokens ~ (batch size, sequence length)\n",
    "        # source_mask ~ (batch size, sequence length)\n",
    "        # target_mask ~ (batch size, sequence length)\n",
    "        # 1) Embed source and run encoder\n",
    "        encoder_output = self._encoder(\n",
    "            self._embeddings[\"source\"](source_tokens),\n",
    "            mask=source_mask\n",
    "        )\n",
    "        # 2) Embed target and run encoder\n",
    "        decoder_output = self._decoder(\n",
    "            self._embeddings[\"target\"](target_tokens),\n",
    "            encoder_output=encoder_output,\n",
    "            mask=target_mask\n",
    "        )\n",
    "        # 3) Project output to vocabulary\n",
    "        output = self._projection(decoder_output)\n",
    "        # 4) Construct output dict\n",
    "        output_dict = {\"logits\": output, \"probs\": torch.softmax(output, dim=-1)}\n",
    "        output_dict[\"prediction\"] = torch.argmax(output_dict[\"probs\"], dim=-1)\n",
    "        output_dict[\"loss\"] = self._loss(\n",
    "            logits=output_dict[\"logits\"],\n",
    "            target=target_tokens,\n",
    "            weights=target_mask.float(),\n",
    "        )\n",
    "        return output_dict\n",
    "\n",
    "    def decode(self, indices: List[int], field: str) -> List[str]:\n",
    "        \"\"\"Decode sequence of `indices` based on `filed` in Vocabulary.\"\"\"\n",
    "        return [self.vocab.index2word[field][int(idx)] for idx in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ниже уже знакомый вам train-evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    return_losses: bool = False,\n",
    ") -> Union[Dict[str, float], Tuple[Dict[str, float], List[float]]]:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    all_losses = []\n",
    "    with tqdm(total=len(data_loader), file=sys.stdout) as prbar:\n",
    "        for batch in data_loader:\n",
    "            # Move Batch to GPU\n",
    "            batch = batch.to_device(model.device, non_blocking=True)\n",
    "            # Get model results\n",
    "            output_dict = model(**batch)\n",
    "            loss = output_dict['loss']\n",
    "            # Update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Update descirption for tqdm\n",
    "            prbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "            prbar.update(1)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            all_losses.append(loss.detach().item())\n",
    "    metrics = {\"loss\": total_loss / num_batches}\n",
    "    if return_losses:\n",
    "        return metrics, all_losses\n",
    "    else:\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def validate(\n",
    "    model: torch.nn.Module,\n",
    "    data_loader: DataLoader,\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    with tqdm(total=len(data_loader), file=sys.stdout) as prbar:\n",
    "        for batch in data_loader:\n",
    "            # Move Batch to GPU\n",
    "            batch = batch.to_device(model.device, non_blocking=True)\n",
    "            # Get model results\n",
    "            output_dict = model(**batch)\n",
    "            loss = output_dict['loss']\n",
    "            # Update descirption for tqdm\n",
    "            prbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "            prbar.update(1)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    metrics = {\"loss\": total_loss / num_batches}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def random_sample_results(\n",
    "    model: torch.nn.Module,\n",
    "    batch: CollateBatch,\n",
    ") -> None:\n",
    "    \"\"\"Randomly sample from batch to log model results.\"\"\"\n",
    "    batch = batch.to_device(model.device, non_blocking=True)\n",
    "    result = model(**batch)\n",
    "    # Get one sample to log\n",
    "    idx = random.randint(0, len(batch))\n",
    "    # Print results\n",
    "    print(\n",
    "        f\"#############\\n\"\n",
    "        f\"RANDOM SAMPLE:\\n\"\n",
    "        f\"Source sample: {' '.join(model.decode(batch['source_tokens'][idx], field='source'))}\\n\"\n",
    "        f\"Target sample: {' '.join(model.decode(batch['target_tokens'][idx], field='target'))}\\n\"\n",
    "        f\"Predict sample: {' '.join(model.decode(result['prediction'][idx], field='target'))}\\n\"\n",
    "        f\"#############\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossInfo(NamedTuple):\n",
    "    full_train_losses: List[float]\n",
    "    train_epoch_losses: List[float]\n",
    "    eval_epoch_losses: List[float]\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model: torch.nn.Module,\n",
    "    epochs: int,\n",
    "    train_data_loader: DataLoader,\n",
    "    validation_data_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    show_random: bool = True\n",
    ") -> LossInfo:\n",
    "    all_train_losses = []\n",
    "    epoch_train_losses = []\n",
    "    epoch_eval_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        # Train step\n",
    "        print(f\"Train Epoch: {epoch}\")\n",
    "        train_metrics, one_epoch_train_losses = train_epoch(\n",
    "            model=model,\n",
    "            data_loader=train_data_loader,\n",
    "            optimizer=optimizer,\n",
    "            return_losses=True,\n",
    "        )\n",
    "        print(f\"Train step loss: {train_metrics['loss']:.4f}\")\n",
    "        # Save Train losses\n",
    "        all_train_losses.extend(one_epoch_train_losses)\n",
    "        epoch_train_losses.append(train_metrics['loss'])\n",
    "        # Eval step\n",
    "        print(f\"Validation Epoch: {epoch}\")\n",
    "        with torch.no_grad():\n",
    "            validation_metrics = validate(\n",
    "                model=model,\n",
    "                data_loader=validation_data_loader,\n",
    "            )\n",
    "            print(f\"Validation step loss: {validation_metrics['loss']:.4f}\")\n",
    "            if show_random:\n",
    "                # Get random batch\n",
    "                batch = next(iter(validation_data_loader))\n",
    "                random_sample_results(model, batch)\n",
    "        # Save eval losses\n",
    "        epoch_eval_losses.append(validation_metrics['loss'])\n",
    "    return LossInfo(all_train_losses, epoch_train_losses, epoch_eval_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ну что же... Перейдём к обучению моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сначала LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала encoder\n",
    "input_size = 128\n",
    "encoder_hidden_size = 256\n",
    "encoder = LSTMEncoder(\n",
    "    input_size=input_size,\n",
    "    hidden_size=encoder_hidden_size,\n",
    "    # Dropout in LSTM Module is activated only if pass num_layers > 1\n",
    "    dropout=0.0,\n",
    ")\n",
    "# Теперь decoder\n",
    "decoder = LSTMDecoder(\n",
    "    input_size=encoder_hidden_size,\n",
    "    hidden_size=encoder_hidden_size,\n",
    "    # Dropout in LSTM Module is activated only if pass num_layers > 1\n",
    "    dropout=0.0,\n",
    ")\n",
    "# Ну и объединим это всё в одну модель\n",
    "lstm_model = NMTModel(\n",
    "    vocab=vocab,\n",
    "    embeddings=torch.nn.ModuleDict({\n",
    "        \"source\": torch.nn.Embedding(vocab.get_size(\"source\"), input_size, padding_idx=0),\n",
    "        \"target\": torch.nn.Embedding(vocab.get_size(\"target\"), encoder_hidden_size, padding_idx=0),\n",
    "    }),\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=DEVICE,\n",
    ").to(device=DEVICE)\n",
    "print(f\"Количество параметров: {sum(p.numel() for p in lstm_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_info = fit(\n",
    "    model=lstm_model,\n",
    "    epochs=EPOCHS,\n",
    "    train_data_loader=train_dataloader,\n",
    "    validation_data_loader=validation_dataloader,\n",
    "    optimizer=torch.optim.Adam(lstm_model.parameters(), lr=LR)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теперь Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала encoder\n",
    "encoder = TransformerEncoder(\n",
    "    input_size=128,\n",
    "    num_layers=1,\n",
    "    feedforward_hidden_dim=512,\n",
    "    num_attention_heads=2,\n",
    ")\n",
    "# Теперь decoder\n",
    "decoder = TransformerDecoder(\n",
    "    input_size=encoder.get_output_size(),\n",
    "    num_layers=1,\n",
    "    feedforward_hidden_dim=512,\n",
    "    num_attention_heads=2,\n",
    ")\n",
    "# Ну и объединим это всё в одну модель\n",
    "transformer_model = NMTModel(\n",
    "    vocab=vocab,\n",
    "    embeddings=torch.nn.ModuleDict({\n",
    "        \"source\": torch.nn.Embedding(vocab.get_size(\"source\"), 128),\n",
    "        \"target\": torch.nn.Embedding(vocab.get_size(\"target\"), 128),\n",
    "    }),\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=DEVICE,\n",
    ").to(device=DEVICE)\n",
    "print(f\"Количество параметров: {sum(p.numel() for p in transformer_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_info = fit(\n",
    "    model=transformer_model,\n",
    "    epochs=EPOCHS,\n",
    "    train_data_loader=train_dataloader,\n",
    "    validation_data_loader=validation_dataloader,\n",
    "    optimizer=torch.optim.Adam(transformer_model.parameters(), lr=LR)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "BERT - это просто несколько stacked Transformer Encoder слоёв из общей модели Transformer + задача Masked Language Modelling.\n",
    "\n",
    "**Зачем нужен вообще Masked Language Modelling?**\n",
    "\n",
    "Оригинально BERT претрейнили на задачу Language Modelling, ну и как можно догадаться язык у нас строится последовательно слово за слово, но обычный Transformer так не умеет, так как это bidirectional модель. Такая архитектура не подходит для Language Modelling в общей постановке, так как каждое слово будет косвенно видеть само себя в контекста. Тогда создатели статьи решили применить задачу Cloze или же Masked Language Modelling.\n",
    "\n",
    "Особенность Masked Language Modelling заключается в том, что мы случайном образом ставим вместо слова **[MASK]** и пытаемся по контексту предсказать тот токен, который там был изначально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE\n",
    "Кроме того, в BERT активно используется такая вещь как Byte Pair Encoding. С помощью него решают проблему очень большого вокабуляра для языка, BPE позволяет регулировать его размер. Грубо говоря BPE сегментация оставляет самые частотные токены в датасете, а редкие разделяет на несколько токенов. Это, как можно догадаться, решает проблему дисбаланаса классов, когда наша модель почти не будет предсказывать редкие слова. Притом BPE потенциально позволяет модели научиться распознавать морфологию языка, композицию слов и даже транслитерацию.\n",
    "* Вот ссылочка на отличную статью на тему BPE: https://arxiv.org/abs/1910.13267\n",
    "* Для BPE я люблю использовать SentencePiece от Google.\n",
    "\n",
    "Тут ещё стоит добавить, что некоторые из вас, читая статью от Google про BERT, могли заметить такую вещь как WordPiece, которую использовали исследователи для BPE токенизации. WordPiece отличается от SentencePiece тем, что строит вокабуляр не частотно как последний, а через максимизацию правдоподбия. Теоретически это должно быть эффективно, однако ребята, написавшие RoBERTa, проверили это и пришли к выводу, что частотного подхода достаточно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT\n",
    "![SegmentLocal](media/gpt-3-example.gif \"GPT-3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iad_seminars_transformers-venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}